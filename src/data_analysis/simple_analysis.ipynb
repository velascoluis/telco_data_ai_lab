{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the required libraries\n",
    "\n",
    "* `seaborn` = Python data visualization\n",
    "* `matplotlib` = creating static, animated and interactive visualizations\n",
    "* `os` = various operating system functionality\n",
    "* `sys` = provides access to variables used by the interpreter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue loading some utilities, feel free to browse the code located under `src/utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils import (\n",
    "    run_query,\n",
    "    generate_sql_query,\n",
    "    load_constants,\n",
    "    explain_chart,\n",
    "    generate_potential_questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `load_constants()` function define the constants to be utilised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = load_constants()\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = constants[\"GCP\"][\"GOOGLE_CLOUD_PROJECT\"]\n",
    "GOOGLE_CLOUD_LOCATION = constants[\"GCP\"][\"GOOGLE_CLOUD_LOCATION\"]\n",
    "GOOGLE_CLOUD_GCS_BUCKET = constants[\"GCP\"][\"GOOGLE_CLOUD_GCS_BUCKET\"]\n",
    "GOOGLE_GEMINI_MODEL_15 = constants[\"VERTEX\"][\"GOOGLE_GEMINI_MODEL_15\"]\n",
    "\n",
    "GOOGLE_CLOUD_BIGQUERY_PROJECT = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_PROJECT\"]\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_DATASET\"]\n",
    "\n",
    "\n",
    "BASE_TABLE_NAME_EVENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_EVENTS\"]\n",
    "BASE_TABLE_NAME_INCIDENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_INCIDENTS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the base tables\n",
    "\n",
    "Navigate to BigQuery, and inspect the tables `telco_rca_events` and `telco_rca_incidents` on the `rca_dataset`.\n",
    "\n",
    "![data_analysis_03](../../assets/data_analysis_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Distance from average\n",
    "\n",
    "Lets calculate the event count distance to the overall event average per each event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"WITH EventCounts AS (\n",
    "    SELECT\n",
    "        event,\n",
    "        COUNT(*) AS event_count\n",
    "    FROM\n",
    "        `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}`\n",
    "    WHERE\n",
    "        event IS NOT NULL\n",
    "    GROUP BY\n",
    "        event\n",
    "),\n",
    "AverageCount AS (\n",
    "    SELECT\n",
    "        AVG(event_count) AS average_count\n",
    "    FROM\n",
    "        EventCounts\n",
    ")\n",
    "SELECT\n",
    "    ec.event,\n",
    "    ec.event_count - ac.average_count AS difference_from_average\n",
    "FROM\n",
    "    EventCounts ec\n",
    "CROSS JOIN\n",
    "    AverageCount ac\n",
    "ORDER BY\n",
    "    difference_from_average;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"difference_from_average\", y=\"event\", data=df)\n",
    "plt.title(\"Difference from Average by Event\")\n",
    "plt.xlabel(\"Difference from Average\")\n",
    "plt.ylabel(\"Event\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"diff_average.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Gemini API multimodal capablities to analyze the generated chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explain_chart(os.path.join(os.getcwd(),\"diff_average.png\"),\n",
    "              GOOGLE_CLOUD_PROJECT,\n",
    "              GOOGLE_CLOUD_LOCATION,\n",
    "              GOOGLE_GEMINI_MODEL_15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are more familiar with `pandas` instead of SQL, you can also use [BigQuery Dataframes](https://cloud.google.com/bigquery/docs/reference/bigquery-dataframes), an open source python package that \"translates\" from pandas dataframe syntax to BQ SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigframes.pandas as bpd\n",
    "bdf = bpd.read_gbq(f\"{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}\")\n",
    "\n",
    "event_counts = bdf[\"event\"].dropna().value_counts().reset_index()\n",
    "event_counts.columns = [\"event\", \"event_count\"]\n",
    "average_count = event_counts[\"event_count\"].mean()\n",
    "\n",
    "result = (\n",
    "    event_counts.assign(\n",
    "        difference_from_average=lambda x: x[\"event_count\"] - average_count\n",
    "    )\n",
    "    .sort_values(\"difference_from_average\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Potential correlations\n",
    "\n",
    "Lets find any potential correlations (Pearson) between the CPU Utilization and Memory Utilization on each type of network element to discard any potential side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"SELECT \n",
    "  cpu_utilization.network_element_id,\n",
    "  ROUND(CORR(cpu_utilization.value, memory_usage.value), 4) AS correlation_coefficient\n",
    "      FROM    `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` cpu_utilization\n",
    "      JOIN    `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` memory_usage \n",
    "  ON cpu_utilization.network_element_id = memory_usage.network_element_id \n",
    "  AND cpu_utilization.timestamp = memory_usage.timestamp\n",
    "WHERE cpu_utilization.metric = 'CPU Utilization'  \n",
    "  AND memory_usage.metric = 'Memory Utilization'\n",
    "  AND cpu_utilization.value is not null \n",
    "  AND memory_usage.value is not null\n",
    "GROUP BY network_element_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3: GenAI query\n",
    "\n",
    "Next, we will use Gemini for a basic NL 2 SQL task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = generate_sql_query(\n",
    "    \"Calculate network elements with the most events\",\n",
    "    f\"{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}\",\n",
    "    GOOGLE_CLOUD_PROJECT,\n",
    "    GOOGLE_CLOUD_LOCATION,\n",
    "    GOOGLE_GEMINI_MODEL_15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try also with the direct integration right in the BigQuery Studio UI\n",
    "\n",
    "![data_analysis_00](../../assets/data_analysis_00.png)\n",
    "\n",
    "![data_analysis_01](../../assets/data_analysis_01.png)\n",
    "\n",
    "![data_analysis_02](../../assets/data_analysis_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4: Try it yourself\n",
    "\n",
    "Generate a query in both SQL and pandas , display a chart and explain it for the following business question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_potential_questions(f\"{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}\",\n",
    "    GOOGLE_CLOUD_PROJECT,\n",
    "    GOOGLE_CLOUD_LOCATION,\n",
    "    GOOGLE_GEMINI_MODEL_15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
