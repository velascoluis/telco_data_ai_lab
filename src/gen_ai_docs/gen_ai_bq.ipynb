{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding semantic search for issue resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart illustrates a workflow for generating solutions to incidents using a combination of Retrieval and Generation techniques, potentially leveraging tools like BQML (BigQuery ML) and LLMs (Large Language Models).\n",
    "\n",
    "### Process Flow\n",
    "\n",
    "1. **Document Processing:**\n",
    "   * **Input:** The process starts with raw incident resolution data,  stored in PDF files, this data is stored on a BigQuery object table.\n",
    "   * **DocAI Processor:** A DocAI Processor is used to extract structured information from the documents. The output is a parsed table containing key details about the incidents. We call DocAI from wthin BigQuery.\n",
    "\n",
    "2. **Retrieval:**\n",
    "   * **Embedding Generation:**\n",
    "     * **BQML GENERATE EMBEDDING:** The incident text (or parsed information) is passed through a BQML model to generate an embedding vector. This vector represents the semantic meaning of the incident in a numerical format.\n",
    "   * **Vector Search:**\n",
    "     * **BQML VECTOR INDEX:** The embedding vector is used to query a BQML VECTOR INDEX . This index stores pre-computed embeddings of existing incident resolutions or knowledge base articles. \n",
    "     * **BQML VECTOR SEARCH:** The search retrieves the top K most similar items (resolutions) from the index based on the similarity between the query embedding and the stored embeddings.\n",
    "\n",
    "3. **Generation:**\n",
    "   * **Prompt Construction:**\n",
    "     * A prompt is created for the LLM. This prompt includes:\n",
    "       * Instruction to produce a solution for the incident\n",
    "       * The retrieved top K resolutions (or their summaries) as context\n",
    "   * **LLM Generation:**\n",
    "     * **BQML GENERATE TEXT:** The LLM processes the prompt and generates a solution (or response) to the incident. The generated text leverages both the information from the retrieved resolutions and the LLM's own language understanding capabilities.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "* **DocAI Processor:** Extracts structured data from unstructured incident documents.\n",
    "* **BQML:**\n",
    "    * **GENERATE EMBEDDING:** Creates embedding vectors representing the semantic meaning of text.\n",
    "    * **VECTOR INDEX:** Stores pre-computed embeddings for efficient similarity search\n",
    "    * **VECTOR SEARCH:** Retrieves similar items from the vector index\n",
    "    * **GENERATE TEXT:**  Generates text using an LLM\n",
    "* **LLM:**  Large Language Model (Gemini) used for generating the final incident solution based on the retrieved context.\n",
    "\n",
    "### Benefits of this Approach\n",
    "\n",
    "* **Leverages Existing Knowledge:** Retrieval from a knowledge base ensures that the generated solution is informed by past experiences and best practices.\n",
    "* **Improves Accuracy:** The LLM's output is grounded in relevant context, reducing the chances of generating irrelevant or inaccurate solutions.\n",
    "* **Efficient:** The vector index allows for fast retrieval of relevant information, even from large knowledge bases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gen_ai_bq_00](../../assets/gen_ai_bq_00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils import run_query, load_constants\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = load_constants()\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = constants[\"GCP\"][\"GOOGLE_CLOUD_PROJECT\"]\n",
    "GOOGLE_CLOUD_LOCATION = constants[\"GCP\"][\"GOOGLE_CLOUD_LOCATION\"]\n",
    "GOOGLE_CLOUD_LOCATION_MULTI_REGION = constants[\"GCP\"][\"GOOGLE_CLOUD_LOCATION_MULTI_REGION\"]\n",
    "GOOGLE_CLOUD_GCS_BUCKET = constants[\"GCP\"][\"GOOGLE_CLOUD_GCS_BUCKET\"]\n",
    "GOOGLE_CLOUD_GCS_BUCKET_MULTI_REGION = constants[\"GCP\"][\n",
    "    \"GOOGLE_CLOUD_GCS_BUCKET_MULTI_REGION\"\n",
    "]\n",
    "GOOGLE_GEMINI_MODEL_15 = constants[\"VERTEX\"][\"GOOGLE_GEMINI_MODEL_15\"]\n",
    "GOOGLE_GEMINI_MODEL_10 = constants[\"VERTEX\"][\"GOOGLE_GEMINI_MODEL_10\"]\n",
    "\n",
    "GOOGLE_CLOUD_BIGQUERY_PROJECT = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_PROJECT\"]\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_DATASET\"]\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION = constants[\"BIGQUERY\"][\n",
    "    \"GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION\"\n",
    "]\n",
    "\n",
    "\n",
    "BASE_TABLE_NAME_EVENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_EVENTS\"]\n",
    "BASE_TABLE_NAME_INCIDENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_INCIDENTS\"]\n",
    "\n",
    "DOC_AI_PROCESSOR_URI = constants[\"DOC_AI\"][\"DOC_AI_PROCESSOR_URI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look to one of our incident resolution documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage cp gs://{GOOGLE_CLOUD_GCS_BUCKET_MULTI_REGION}/rca/incident_resolution_20.pdf ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"incident_resolution_20.pdf\", width=800, height=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_cext = f\"\"\"CREATE OR REPLACE EXTERNAL TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs`\n",
    "  WITH CONNECTION `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_LOCATION_MULTI_REGION}.genai`\n",
    "  OPTIONS (\n",
    "    object_metadata = 'SIMPLE',\n",
    "    uris = ['gs://{GOOGLE_CLOUD_GCS_BUCKET_MULTI_REGION}/rca/*'],\n",
    "    metadata_cache_mode= 'AUTOMATIC',\n",
    "    max_staleness= INTERVAL 1 HOUR\n",
    "  );\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_cmodel = f\"\"\"\n",
    "  CREATE OR REPLACE MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.rca_processor`\n",
    "  REMOTE WITH CONNECTION `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_LOCATION_MULTI_REGION}.genai`\n",
    "  OPTIONS (\n",
    "    remote_service_type = 'CLOUD_AI_DOCUMENT_V1',   \n",
    "    document_processor='{DOC_AI_PROCESSOR_URI}'\n",
    "  );\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_cext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parse = f\"\"\"\n",
    "  CREATE OR REPLACE TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs_parsed` AS\n",
    "  SELECT *\n",
    "  FROM ML.PROCESS_DOCUMENT(\n",
    "    MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.rca_processor`,\n",
    "    TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs`)\n",
    "  WHERE content_type = 'application/pdf';\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emodel = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gecko_embedder`\n",
    "  REMOTE WITH CONNECTION `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_LOCATION_MULTI_REGION}.genai`\n",
    "  OPTIONS (ENDPOINT = \"textembedding-gecko-multilingual\");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_emodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_genembs = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs_embedded` AS\n",
    "SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "  MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gecko_embedder`,\n",
    "  (\n",
    "    SELECT  JSON_VALUE(ml_process_document_result, '$.text') AS content, uri as title\n",
    "    FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs_parsed`\n",
    "  )\n",
    ")\n",
    "WHERE LENGTH(ml_generate_embedding_status) = 0;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_genembs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emodel = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gemini_model`\n",
    "  REMOTE WITH CONNECTION `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_LOCATION_MULTI_REGION}.genai`\n",
    "  OPTIONS (ENDPOINT = '{GOOGLE_GEMINI_MODEL_10}');\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_emodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'Im having a high CPU utilization incident together with  Network Congestion Alert and High Active Connection Count Alert'\n",
    "query_search = f\"\"\"\n",
    "SELECT *\n",
    "FROM VECTOR_SEARCH(\n",
    "  TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs_embedded`, 'ml_generate_embedding_result',\n",
    "  (\n",
    "  SELECT ml_generate_embedding_result, content AS query\n",
    "  FROM ML.GENERATE_EMBEDDING(\n",
    "   MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gecko_embedder`,\n",
    "  (SELECT '{user_query}' AS content))\n",
    "  ),\n",
    "  top_k => 5);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(query_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Im having a high CPU utilization incident together with  Network Congestion Alert and High Active Connection Count Alert\"\n",
    "query_rag = f\"\"\"SELECT ml_generate_text_result.candidates[0].content.parts[0].text\n",
    "FROM ML.GENERATE_TEXT(\n",
    "  MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gemini_model`,\n",
    "  (\n",
    "    SELECT CONCAT(\n",
    "      'Detail how to solve the issue using the following articles, produce a step by step guide ',\n",
    "      STRING_AGG(base.content)\n",
    "      ) AS prompt,\n",
    "    FROM VECTOR_SEARCH(\n",
    "  TABLE `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.{BASE_TABLE_NAME_INCIDENTS}_docs_embedded`, 'ml_generate_embedding_result',\n",
    "  (\n",
    "  SELECT ml_generate_embedding_result, content AS query\n",
    "  FROM ML.GENERATE_EMBEDDING(\n",
    "   MODEL `{GOOGLE_CLOUD_BIGQUERY_DATASET_MULTI_REGION}.gecko_embedder`,\n",
    "  (SELECT '{user_query}' AS content))\n",
    "  ),\n",
    "  top_k => 10)\n",
    "  ), STRUCT(8192 as max_output_tokens));\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_query(query_rag)['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also included a simple webapp to see this lab in an interactive manner.\n",
    "\n",
    "Go back to the Google Cloud Console and open a Cloud Shell, execute the following commands from the terminal.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/velascoluis/telco_data_ai_lab.git\n",
    "cd telco_data_ai_lab/src/gen_ai_docs/webapp\n",
    "source launch_local_test.sh\n",
    "```\n",
    "Click on web preview to display the webapp on the browser\n",
    "\n",
    "![gen_ai_bq_01](../../assets/gen_ai_bq_01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
