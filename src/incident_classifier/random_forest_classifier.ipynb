{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple incident classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils import run_query, load_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = load_constants()\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = constants[\"GCP\"][\"GOOGLE_CLOUD_PROJECT\"]\n",
    "GOOGLE_CLOUD_LOCATION = constants[\"GCP\"][\"GOOGLE_CLOUD_LOCATION\"]\n",
    "GOOGLE_CLOUD_GCS_BUCKET = constants[\"GCP\"][\"GOOGLE_CLOUD_GCS_BUCKET\"]\n",
    "GOOGLE_CLOUD_SERVICE_ACCOUNT = constants[\"GCP\"][\"GOOGLE_CLOUD_SERVICE_ACCOUNT\"]\n",
    "GOOGLE_GEMINI_MODEL_15 = constants[\"VERTEX\"][\"GOOGLE_GEMINI_MODEL_15\"]\n",
    "\n",
    "GOOGLE_CLOUD_BIGQUERY_PROJECT = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_PROJECT\"]\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_DATASET\"]\n",
    "\n",
    "\n",
    "BASE_TABLE_NAME_EVENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_EVENTS\"]\n",
    "BASE_TABLE_NAME_INCIDENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_INCIDENTS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` TABLESAMPLE SYSTEM (10 PERCENT) \n",
    "\"\"\"\n",
    "events_df = run_query(events_query)\n",
    "\n",
    "incidents_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_INCIDENTS}`\n",
    "\"\"\"\n",
    "incidents_df = run_query(incidents_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, window_size=\"1h\"):\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\")\n",
    "\n",
    "    features = df.groupby([\"network_element_id\", pd.Grouper(freq=window_size)]).agg(\n",
    "        mean_value=(\"value\", \"mean\"),\n",
    "        max_value=(\"value\", \"max\"),\n",
    "        min_value=(\"value\", \"min\"),\n",
    "        count_events=(\"event\", \"count\"))\n",
    "    features = features.reset_index()\n",
    "    network_wide = df.groupby(pd.Grouper(freq=window_size)).agg(\n",
    "        network_mean_value=(\"value\", \"mean\"),\n",
    "        network_max_value=(\"value\", \"max\"),\n",
    "        network_min_value=(\"value\", \"min\"),\n",
    "        network_count_events=(\"event\", \"count\"))\n",
    "    network_wide = network_wide.reset_index()\n",
    "    features = pd.merge(features, network_wide, on=\"timestamp\", how=\"left\")\n",
    "    element_ids = df[\"network_element_id\"].unique()\n",
    "\n",
    "    for element1, element2 in combinations(element_ids, 2):\n",
    "        features_e1 = features[features['network_element_id'] == element1]\n",
    "        features_e2 = features[features['network_element_id'] == element2]\n",
    "        merged = pd.merge(features_e1, features_e2, on='timestamp', how='left', suffixes=('_e1', '_e2'))\n",
    "        merged['mean_diff_e1_e2'] = merged['mean_value_e1'] - merged['mean_value_e2']\n",
    "        merged = merged.rename(columns={'mean_diff_e1_e2': f'mean_diff_{element1}_{element2}'})\n",
    "        features = pd.merge(\n",
    "            features,\n",
    "            merged[['timestamp', 'network_element_id_e1', f'mean_diff_{element1}_{element2}']],\n",
    "            left_on=['timestamp', 'network_element_id'],\n",
    "            right_on=['timestamp', 'network_element_id_e1'],\n",
    "            how='left',\n",
    "        )\n",
    "        features = features.drop('network_element_id_e1', axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def join_with_incidents(features_df, incidents_df):\n",
    "    df = features_df.copy()\n",
    "    df[\"incident_occurred\"] = 0  \n",
    "    for _, row in incidents_df.iterrows():\n",
    "        start_time = row[\"start_time\"]\n",
    "        end_time = row[\"end_time\"]\n",
    "        incident_name = row[\"incident_name\"]\n",
    "        matching_features = df[\n",
    "            (df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] <= end_time)\n",
    "        ]\n",
    "        \n",
    "        df.loc[matching_features.index, \"incident_occurred\"] = 1\n",
    "        df.loc[matching_features.index, \"incident_name\"] = incident_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_features = create_features(events_df)\n",
    "events_with_incidents = join_with_incidents(events_features, incidents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "events_with_incidents[\"network_element_id\"] = le.fit_transform(\n",
    "    events_with_incidents[\"network_element_id\"]\n",
    ")\n",
    "\n",
    "features = [\n",
    "    \"network_element_id\",\n",
    "    \"mean_value\",\n",
    "    \"max_value\",\n",
    "    \"min_value\",\n",
    "    \"count_events\",\n",
    "    \"network_mean_value\",\n",
    "    \"network_max_value\",\n",
    "    \"network_min_value\",\n",
    "    \"network_count_events\",\n",
    "]\n",
    "\n",
    "for element1, element2 in combinations(events_df[\"network_element_id\"].unique(), 2):\n",
    "    features.append(f\"mean_diff_{element1}_{element2}\")\n",
    "\n",
    "target = \"incident_occurred\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = events_with_incidents[features]\n",
    "y = events_with_incidents[target]\n",
    "\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset,InputPath, Model,OutputPath, component)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\", \n",
    ")\n",
    "def load_data_from_bigquery_op(query: str, project_id: str, output_data_path: OutputPath(Dataset)):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    df.to_csv(output_data_path, index=False)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def create_features_op(\n",
    "    input_data_path: InputPath(Dataset),\n",
    "    output_data_path: OutputPath(Dataset),\n",
    "    window_size: str = '1h',\n",
    "):\n",
    "    import pandas as pd\n",
    "    from itertools import combinations\n",
    "\n",
    "    def create_features(df, window_size=\"1h\"):\n",
    "        df = df.copy()\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df = df.set_index(\"timestamp\")\n",
    "\n",
    "        features = df.groupby([\"network_element_id\", pd.Grouper(freq=window_size)]).agg(\n",
    "            mean_value=(\"value\", \"mean\"),\n",
    "            max_value=(\"value\", \"max\"),\n",
    "            min_value=(\"value\", \"min\"),\n",
    "            count_events=(\"event\", \"count\"),\n",
    "        )\n",
    "        features = features.reset_index()\n",
    "\n",
    "        network_wide = df.groupby(pd.Grouper(freq=window_size)).agg(\n",
    "            network_mean_value=(\"value\", \"mean\"),\n",
    "            network_max_value=(\"value\", \"max\"),\n",
    "            network_min_value=(\"value\", \"min\"),\n",
    "            network_count_events=(\"event\", \"count\"),\n",
    "        )\n",
    "        network_wide = network_wide.reset_index()\n",
    "\n",
    "        features = pd.merge(features, network_wide, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "        element_ids = df[\"network_element_id\"].unique()\n",
    "        for element1, element2 in combinations(element_ids, 2):\n",
    "            features_e1 = features[features[\"network_element_id\"] == element1]\n",
    "            features_e2 = features[features[\"network_element_id\"] == element2]\n",
    "            merged = pd.merge(\n",
    "                features_e1,\n",
    "                features_e2,\n",
    "                on=\"timestamp\",\n",
    "                how=\"left\",\n",
    "                suffixes=(\"_e1\", \"_e2\"),\n",
    "            )\n",
    "            merged[\"mean_diff_e1_e2\"] = merged[\"mean_value_e1\"] - merged[\"mean_value_e2\"]\n",
    "\n",
    "            merged = merged.rename(\n",
    "                columns={\"mean_diff_e1_e2\": f\"mean_diff_{element1}_{element2}\"}\n",
    "            )\n",
    "\n",
    "            features = pd.merge(\n",
    "                features,\n",
    "                merged[\n",
    "                    [\n",
    "                        \"timestamp\",\n",
    "                        \"network_element_id_e1\",\n",
    "                        f\"mean_diff_{element1}_{element2}\",\n",
    "                    ]\n",
    "                ],\n",
    "                left_on=[\"timestamp\", \"network_element_id\"],\n",
    "                right_on=[\"timestamp\", \"network_element_id_e1\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            features = features.drop(\"network_element_id_e1\", axis=1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    features_df = create_features(df, window_size)\n",
    "    features_df.to_csv(output_data_path, index=False)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-bigquery\", \"google-cloud-storage\",\"db-dtypes\"],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def join_features_op(\n",
    "    features_data_path: InputPath(Dataset),\n",
    "    incidents_data_path: InputPath(Dataset),\n",
    "    output_data_path: OutputPath(Dataset),\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    def join_with_incidents(features_df, incidents_df):\n",
    "        df = features_df.copy()\n",
    "        df[\"incident_occurred\"] = 0\n",
    "        for _, row in incidents_df.iterrows():\n",
    "            start_time = row[\"start_time\"]\n",
    "            end_time = row[\"end_time\"]\n",
    "            incident_name = row[\"incident_name\"]\n",
    "\n",
    "            matching_features = df[\n",
    "                (df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] <= end_time)\n",
    "            ]\n",
    "\n",
    "            df.loc[matching_features.index, \"incident_occurred\"] = 1\n",
    "            df.loc[matching_features.index, \"incident_name\"] = incident_name\n",
    "        return df\n",
    "\n",
    "    features_df = pd.read_csv(features_data_path)\n",
    "    incidents_df = pd.read_csv(incidents_data_path)\n",
    "    joined_df = join_with_incidents(features_df, incidents_df)\n",
    "    joined_df.to_csv(output_data_path, index=False)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-bigquery\", \"google-cloud-storage\", \"joblib\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.10\", \n",
    ")\n",
    "def train_and_evaluate_model_op(\n",
    "    input_data_path: InputPath(Dataset),\n",
    "    model_output_path: OutputPath(Model),\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from joblib import dump\n",
    "    from itertools import combinations\n",
    "\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    le = LabelEncoder()\n",
    "    df['network_element_id'] = le.fit_transform(df['network_element_id'])\n",
    "    target = 'incident_occurred'\n",
    "    X = df.drop(columns=[target,'timestamp','incident_name'])\n",
    "    y = df[target]\n",
    "\n",
    "    X.fillna(0, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    dump(model, model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"telco-incident-prediction-pipeline\",\n",
    "    pipeline_root=f\"gs://{GOOGLE_CLOUD_GCS_BUCKET}/pipeline-root-rca\",\n",
    ")\n",
    "def telco_incident_prediction_pipeline():\n",
    "\n",
    "    load_events_task = load_data_from_bigquery_op(\n",
    "        project_id=GOOGLE_CLOUD_BIGQUERY_PROJECT,\n",
    "        query=f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` TABLESAMPLE SYSTEM (10 PERCENT) \n",
    "        \"\"\",\n",
    "    )\n",
    "    load_incidents_task = load_data_from_bigquery_op(\n",
    "        project_id=GOOGLE_CLOUD_BIGQUERY_PROJECT,\n",
    "        query=f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_INCIDENTS}`\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    create_features_task = create_features_op(\n",
    "        input_data_path=load_events_task.outputs[\"output_data_path\"],\n",
    "        window_size=\"1h\",\n",
    "    )\n",
    "\n",
    "    join_features_task = join_features_op(\n",
    "        features_data_path=create_features_task.outputs[\"output_data_path\"],\n",
    "        incidents_data_path=load_incidents_task.outputs[\"output_data_path\"],\n",
    "    )\n",
    "\n",
    "    train_model_task = train_and_evaluate_model_op(\n",
    "        input_data_path=join_features_task.outputs[\"output_data_path\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=telco_incident_prediction_pipeline,\n",
    "    package_path=\"telco_incident_prediction_pipeline.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "DISPLAY_NAME = \"telco_incident_prediction_pipeline\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=f\"{DISPLAY_NAME}.yml\",\n",
    ")\n",
    "\n",
    "job.run(service_account=GOOGLE_CLOUD_SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
