{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple incident classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will create a simple incident classifier using the Random Forest algorithm from the `sklearn` library.\n",
    "\n",
    "A random forest classifier is a machine learning algorithm that combines the predictions of multiple decision trees for improved accuracy. It uses bagging and feature randomness to create a diverse set of trees. Each tree makes a prediction, and the final prediction is determined by majority voting (for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils import run_query, load_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `load_constants()` function define the constants to be utilised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = load_constants()\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = constants[\"GCP\"][\"GOOGLE_CLOUD_PROJECT\"]\n",
    "GOOGLE_CLOUD_LOCATION = constants[\"GCP\"][\"GOOGLE_CLOUD_LOCATION\"]\n",
    "GOOGLE_CLOUD_GCS_BUCKET = constants[\"GCP\"][\"GOOGLE_CLOUD_GCS_BUCKET\"]\n",
    "GOOGLE_CLOUD_SERVICE_ACCOUNT = constants[\"GCP\"][\"GOOGLE_CLOUD_SERVICE_ACCOUNT\"]\n",
    "GOOGLE_GEMINI_MODEL_15 = constants[\"VERTEX\"][\"GOOGLE_GEMINI_MODEL_15\"]\n",
    "\n",
    "GOOGLE_CLOUD_BIGQUERY_PROJECT = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_PROJECT\"]\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET = constants[\"BIGQUERY\"][\"GOOGLE_CLOUD_BIGQUERY_DATASET\"]\n",
    "\n",
    "\n",
    "BASE_TABLE_NAME_EVENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_EVENTS\"]\n",
    "BASE_TABLE_NAME_INCIDENTS = constants[\"BIGQUERY\"][\"BASE_TABLE_NAME_INCIDENTS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` TABLESAMPLE SYSTEM (10 PERCENT) \n",
    "\"\"\"\n",
    "events_df = run_query(events_query)\n",
    "\n",
    "incidents_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_INCIDENTS}`\n",
    "\"\"\"\n",
    "incidents_df = run_query(incidents_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet performs two main tasks:\n",
    "\n",
    "1. **Feature Creation (create_features function)**\n",
    "\n",
    "   * **Aggregation:** It calculates various statistics (mean, max, min, count) for both individual network elements and the entire network over specified time windows (default: 1 hour).\n",
    "   * **Network-Wide Context:** It adds network-wide statistics to each network element's data, providing context.\n",
    "   * **Pairwise Differences:** It calculates mean value differences between all possible pairs of network elements, creating additional features.\n",
    "\n",
    "2. **Incident Labeling (join_with_incidents function)**\n",
    "\n",
    "   * **Incident Matching:** It checks if each timestamp in the feature data falls within any incident's start and end time.\n",
    "   * **Labeling:** It adds two columns:\n",
    "      * `incident_occurred`: Set to 1 if an incident occurred at that timestamp, 0 otherwise.\n",
    "      * `incident_name`: The name of the incident (if any) at that timestamp.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Time-Based Analysis:** The code uses time windows to aggregate data and align features with incidents.\n",
    "* **Network Element Comparisons:** Pairwise differences help capture relationships between network elements.\n",
    "* **Incident Context:** The `incident_occurred` and `incident_name` columns provide valuable context for further analysis or modeling, potentially linking network behavior to incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, window_size=\"1h\"):\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\")\n",
    "\n",
    "    features = df.groupby([\"network_element_id\", pd.Grouper(freq=window_size)]).agg(\n",
    "        mean_value=(\"value\", \"mean\"),\n",
    "        max_value=(\"value\", \"max\"),\n",
    "        min_value=(\"value\", \"min\"),\n",
    "        count_events=(\"event\", \"count\"))\n",
    "    features = features.reset_index()\n",
    "    network_wide = df.groupby(pd.Grouper(freq=window_size)).agg(\n",
    "        network_mean_value=(\"value\", \"mean\"),\n",
    "        network_max_value=(\"value\", \"max\"),\n",
    "        network_min_value=(\"value\", \"min\"),\n",
    "        network_count_events=(\"event\", \"count\"))\n",
    "    network_wide = network_wide.reset_index()\n",
    "    features = pd.merge(features, network_wide, on=\"timestamp\", how=\"left\")\n",
    "    element_ids = df[\"network_element_id\"].unique()\n",
    "\n",
    "    for element1, element2 in combinations(element_ids, 2):\n",
    "        features_e1 = features[features['network_element_id'] == element1]\n",
    "        features_e2 = features[features['network_element_id'] == element2]\n",
    "        merged = pd.merge(features_e1, features_e2, on='timestamp', how='left', suffixes=('_e1', '_e2'))\n",
    "        merged['mean_diff_e1_e2'] = merged['mean_value_e1'] - merged['mean_value_e2']\n",
    "        merged = merged.rename(columns={'mean_diff_e1_e2': f'mean_diff_{element1}_{element2}'})\n",
    "        features = pd.merge(\n",
    "            features,\n",
    "            merged[['timestamp', 'network_element_id_e1', f'mean_diff_{element1}_{element2}']],\n",
    "            left_on=['timestamp', 'network_element_id'],\n",
    "            right_on=['timestamp', 'network_element_id_e1'],\n",
    "            how='left',\n",
    "        )\n",
    "        features = features.drop('network_element_id_e1', axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def join_with_incidents(features_df, incidents_df):\n",
    "    df = features_df.copy()\n",
    "    df[\"incident_occurred\"] = 0  \n",
    "    for _, row in incidents_df.iterrows():\n",
    "        start_time = row[\"start_time\"]\n",
    "        end_time = row[\"end_time\"]\n",
    "        incident_name = row[\"incident_name\"]\n",
    "        matching_features = df[\n",
    "            (df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] <= end_time)\n",
    "        ]\n",
    "        \n",
    "        df.loc[matching_features.index, \"incident_occurred\"] = 1\n",
    "        df.loc[matching_features.index, \"incident_name\"] = incident_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_features = create_features(events_df)\n",
    "events_with_incidents = join_with_incidents(events_features, incidents_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prepares the data for machine learning modeling:\n",
    "\n",
    "1. **Label Encoding:**\n",
    "\n",
    "* The `LabelEncoder` converts the categorical `network_element_id` into numerical labels. This is often necessary for machine learning algorithms that require numerical input.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "\n",
    "* A list named `features` is created, specifying the columns that will be used as input features for the model.\n",
    "* This includes:\n",
    "    * Statistics for individual network elements (`mean_value`, `max_value`, `min_value`, `count_events`)\n",
    "    * Network-wide statistics (`network_mean_value`, `network_max_value`, `network_min_value`, `network_count_events`)\n",
    "    * Mean value differences between pairs of network elements.\n",
    "\n",
    "3. **Target Variable Definition:**\n",
    "\n",
    "* The `target` variable is set to `incident_occurred`. This indicates that the model will be trained to predict whether an incident occurred based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "events_with_incidents[\"network_element_id\"] = le.fit_transform(\n",
    "    events_with_incidents[\"network_element_id\"]\n",
    ")\n",
    "\n",
    "features = [\n",
    "    \"network_element_id\",\n",
    "    \"mean_value\",\n",
    "    \"max_value\",\n",
    "    \"min_value\",\n",
    "    \"count_events\",\n",
    "    \"network_mean_value\",\n",
    "    \"network_max_value\",\n",
    "    \"network_min_value\",\n",
    "    \"network_count_events\",\n",
    "]\n",
    "\n",
    "for element1, element2 in combinations(events_df[\"network_element_id\"].unique(), 2):\n",
    "    features.append(f\"mean_diff_{element1}_{element2}\")\n",
    "\n",
    "target = \"incident_occurred\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we prepare the data for training and evaluating a machine learning model:\n",
    "\n",
    "1. **Feature and Target Separation:**\n",
    "\n",
    "* `X`: The selected features are extracted from the `events_with_incidents` dataframe and assigned to `X`.\n",
    "* `y`: The target variable (`incident_occurred`) is extracted and assigned to `y`.\n",
    "\n",
    "2. **Handling Missing Values:**\n",
    "\n",
    "* `X.fillna(0, inplace=True)`: Any missing values in the feature data `X` are filled with zeros. This is a common preprocessing step to ensure compatibility with many machine learning algorithms.\n",
    "\n",
    "3. **Train-Test Split:**\n",
    "\n",
    "* `train_test_split`: The data is split into training and testing sets. \n",
    "    * 80% of the data (`test_size=0.2`) is used for training the model (`X_train`, `y_train`).\n",
    "    * 20% is held out for evaluating the model's performance on unseen data (`X_test`, `y_test`).\n",
    "    * `random_state=42` ensures reproducibility of the split.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Model Readiness:** The data is now structured into the standard format for training and evaluating supervised machine learning models: input features (`X`) and corresponding target values (`y`).\n",
    "* **Train-Test Split:** This crucial step helps prevent overfitting by evaluating the model on data it hasn't seen during training, providing a more realistic estimate of its performance on new, unseen data.\n",
    "* **Missing Value Handling:** Filling missing values with zeros is a simple imputation strategy. Depending on the data and the model, other strategies might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = events_with_incidents[features]\n",
    "y = events_with_incidents[target]\n",
    "\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes and trains a Random Forest Classifier model:\n",
    "\n",
    "1. **Model Initialization:**\n",
    "\n",
    "* `model = RandomForestClassifier(n_estimators=100, random_state=42)`: \n",
    "   * A Random Forest Classifier model is created.\n",
    "   * `n_estimators=100`: The model will consist of an ensemble of 100 decision trees.\n",
    "   * `random_state=42`: Sets the random seed for reproducibility, ensuring that the same model is created each time the code is run.\n",
    "\n",
    "2. **Model Training:**\n",
    "\n",
    "* `model.fit(X_train, y_train)`:\n",
    "   * The model is trained on the training data (`X_train`, `y_train`).\n",
    "   * During training, the model learns patterns and relationships between the input features and the target variable (`incident_occurred`). \n",
    "   * These learned patterns are stored within the model's internal structure, allowing it to make predictions on new, unseen data. \n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Random Forest:** A powerful ensemble learning algorithm that often achieves high accuracy and robustness.\n",
    "* **Hyperparameters:** The `n_estimators` parameter controls the number of trees in the forest. Other hyperparameters can be tuned to further optimize the model's performance.\n",
    "* **Training Process:** The `fit` method adjusts the model's internal parameters based on the training data to minimize prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code evaluates the performance of the trained Random Forest Classifier:\n",
    "\n",
    "1. **Prediction:**\n",
    "\n",
    "* `y_pred = model.predict(X_test)`:\n",
    "    * The trained model is used to make predictions on the test data (`X_test`).\n",
    "    * The `predict` method applies the learned patterns from the training phase to generate predicted labels (`y_pred`) for each instance in the test set.\n",
    "\n",
    "2. **Accuracy Calculation:**\n",
    "\n",
    "* `accuracy = accuracy_score(y_test, y_pred)`:\n",
    "    * The `accuracy_score` function compares the predicted labels (`y_pred`) with the true labels (`y_test`).\n",
    "    * It calculates the accuracy, which is the proportion of correctly predicted instances out of the total number of instances in the test set.\n",
    "\n",
    "3. **Result Output:**\n",
    "\n",
    "* `print(f\"Accuracy: {accuracy}\")`:\n",
    "    * The calculated accuracy is printed to the console.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Model Performance:** The accuracy score provides a basic measure of how well the model generalizes to unseen data. \n",
    "* **Evaluation Metric:** Accuracy is suitable for balanced datasets where the classes are roughly equally represented. For imbalanced datasets, other metrics like precision, recall, or F1-score might be more informative.\n",
    "* **Further Analysis:** Depending on the specific problem and requirements, further evaluation and analysis might involve techniques like confusion matrices, ROC curves, or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Vertex AI Pipelines?\n",
    "\n",
    "Vertex AI Pipelines is a managed service on Google Cloud Platform that simplifies the orchestration and execution of machine learning workflows. It's built on Kubeflow Pipelines, providing a scalable and reliable way to:\n",
    "\n",
    "* **Build complex ML workflows:** Chain together multiple ML tasks (data preprocessing, feature engineering, model training, evaluation, etc.)\n",
    "* **Automate & schedule pipelines:** Trigger pipelines on a schedule or based on events\n",
    "* **Track experiments & lineage:** Keep track of model versions, parameters, and data used for reproducibility\n",
    "* **Deploy models to production:** Integrate with Vertex AI for easy model deployment and serving\n",
    "\n",
    "**Key Benefits**\n",
    "\n",
    "* **Managed service:** Reduces operational overhead compared to self-managing Kubeflow\n",
    "* **Integration with Google Cloud:** Seamlessly works with other GCP services (BigQuery, Cloud Storage, etc.)\n",
    "* **Scalability:** Handles large-scale ML workflows\n",
    "\n",
    "**In the context of this code:** These components are designed to run as part of a Vertex AI Pipeline, leveraging its capabilities for orchestration, tracking, and potential scaling of the ML workflow.\n",
    "\n",
    "### Kubeflow Pipeline Components for Incident Prediction\n",
    "\n",
    "This code defines several Kubeflow Pipeline components designed to work together to build and train a machine learning model for incident prediction.\n",
    "\n",
    "#### Components Breakdown\n",
    "\n",
    "* **`load_data_from_bigquery_op`**\n",
    "\n",
    "  * **Purpose:** Loads data from BigQuery using a provided SQL query.\n",
    "  * **Inputs:** `query` (SQL query string), `project_id`\n",
    "  * **Output:** CSV file saved to `output_data_path`\n",
    "\n",
    "* **`create_features_op`**\n",
    "\n",
    "  * **Purpose:** Engineers features from the loaded data.\n",
    "  * **Inputs:** Data from `input_data_path`, `window_size` (time window for aggregation, default 1 hour)\n",
    "  * **Output:** CSV file with engineered features saved to `output_data_path`\n",
    "  * **Key operations:**\n",
    "    * Time-based aggregation of metrics (mean, max, min, count) for each network element\n",
    "    * Calculation of network-wide statistics\n",
    "    * Creation of pairwise difference features between network elements\n",
    "\n",
    "* **`join_features_op`**\n",
    "\n",
    "  * **Purpose:** Joins the engineered features with incident data\n",
    "  * **Inputs:** Feature data from `features_data_path`, incident data from `incidents_data_path`\n",
    "  * **Output:** CSV file with joined data saved to `output_data_path`\n",
    "  * **Key operation:**\n",
    "    * Labels each timestamp in the feature data based on whether an incident occurred during that time\n",
    "\n",
    "* **`train_and_evaluate_model_op`**\n",
    "\n",
    "  * **Purpose:** Trains and evaluates a machine learning model\n",
    "  * **Inputs:** Joined data from `input_data_path`\n",
    "  * **Outputs:** Trained model saved to `model_output_path`\n",
    "  * **Key operations:**\n",
    "    * Preprocessing: Label encodes categorical variables, handles missing values, splits data into training and testing sets\n",
    "    * Trains a Random Forest Classifier\n",
    "    * Evaluates the model on the test set (calculates accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset,InputPath, Model,OutputPath, component)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\", \n",
    ")\n",
    "def load_data_from_bigquery_op(query: str, project_id: str, output_data_path: OutputPath(Dataset)):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    df.to_csv(output_data_path, index=False)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def create_features_op(\n",
    "    input_data_path: InputPath(Dataset),\n",
    "    output_data_path: OutputPath(Dataset),\n",
    "    window_size: str = '1h',\n",
    "):\n",
    "    import pandas as pd\n",
    "    from itertools import combinations\n",
    "\n",
    "    def create_features(df, window_size=\"1h\"):\n",
    "        df = df.copy()\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df = df.set_index(\"timestamp\")\n",
    "\n",
    "        features = df.groupby([\"network_element_id\", pd.Grouper(freq=window_size)]).agg(\n",
    "            mean_value=(\"value\", \"mean\"),\n",
    "            max_value=(\"value\", \"max\"),\n",
    "            min_value=(\"value\", \"min\"),\n",
    "            count_events=(\"event\", \"count\"),\n",
    "        )\n",
    "        features = features.reset_index()\n",
    "\n",
    "        network_wide = df.groupby(pd.Grouper(freq=window_size)).agg(\n",
    "            network_mean_value=(\"value\", \"mean\"),\n",
    "            network_max_value=(\"value\", \"max\"),\n",
    "            network_min_value=(\"value\", \"min\"),\n",
    "            network_count_events=(\"event\", \"count\"),\n",
    "        )\n",
    "        network_wide = network_wide.reset_index()\n",
    "\n",
    "        features = pd.merge(features, network_wide, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "        element_ids = df[\"network_element_id\"].unique()\n",
    "        for element1, element2 in combinations(element_ids, 2):\n",
    "            features_e1 = features[features[\"network_element_id\"] == element1]\n",
    "            features_e2 = features[features[\"network_element_id\"] == element2]\n",
    "            merged = pd.merge(\n",
    "                features_e1,\n",
    "                features_e2,\n",
    "                on=\"timestamp\",\n",
    "                how=\"left\",\n",
    "                suffixes=(\"_e1\", \"_e2\"),\n",
    "            )\n",
    "            merged[\"mean_diff_e1_e2\"] = merged[\"mean_value_e1\"] - merged[\"mean_value_e2\"]\n",
    "\n",
    "            merged = merged.rename(\n",
    "                columns={\"mean_diff_e1_e2\": f\"mean_diff_{element1}_{element2}\"}\n",
    "            )\n",
    "\n",
    "            features = pd.merge(\n",
    "                features,\n",
    "                merged[\n",
    "                    [\n",
    "                        \"timestamp\",\n",
    "                        \"network_element_id_e1\",\n",
    "                        f\"mean_diff_{element1}_{element2}\",\n",
    "                    ]\n",
    "                ],\n",
    "                left_on=[\"timestamp\", \"network_element_id\"],\n",
    "                right_on=[\"timestamp\", \"network_element_id_e1\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            features = features.drop(\"network_element_id_e1\", axis=1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    features_df = create_features(df, window_size)\n",
    "    features_df.to_csv(output_data_path, index=False)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-bigquery\", \"google-cloud-storage\",\"db-dtypes\"],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def join_features_op(\n",
    "    features_data_path: InputPath(Dataset),\n",
    "    incidents_data_path: InputPath(Dataset),\n",
    "    output_data_path: OutputPath(Dataset),\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    def join_with_incidents(features_df, incidents_df):\n",
    "        df = features_df.copy()\n",
    "        df[\"incident_occurred\"] = 0\n",
    "        for _, row in incidents_df.iterrows():\n",
    "            start_time = row[\"start_time\"]\n",
    "            end_time = row[\"end_time\"]\n",
    "            incident_name = row[\"incident_name\"]\n",
    "\n",
    "            matching_features = df[\n",
    "                (df[\"timestamp\"] >= start_time) & (df[\"timestamp\"] <= end_time)\n",
    "            ]\n",
    "\n",
    "            df.loc[matching_features.index, \"incident_occurred\"] = 1\n",
    "            df.loc[matching_features.index, \"incident_name\"] = incident_name\n",
    "        return df\n",
    "\n",
    "    features_df = pd.read_csv(features_data_path)\n",
    "    incidents_df = pd.read_csv(incidents_data_path)\n",
    "    joined_df = join_with_incidents(features_df, incidents_df)\n",
    "    joined_df.to_csv(output_data_path, index=False)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-bigquery\", \"google-cloud-storage\", \"joblib\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.10\", \n",
    ")\n",
    "def train_and_evaluate_model_op(\n",
    "    input_data_path: InputPath(Dataset),\n",
    "    model_output_path: OutputPath(Model),\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from joblib import dump\n",
    "    from itertools import combinations\n",
    "\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    le = LabelEncoder()\n",
    "    df['network_element_id'] = le.fit_transform(df['network_element_id'])\n",
    "    target = 'incident_occurred'\n",
    "    X = df.drop(columns=[target,'timestamp','incident_name'])\n",
    "    y = df[target]\n",
    "\n",
    "    X.fillna(0, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    dump(model, model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"telco-incident-prediction-pipeline\",\n",
    "    pipeline_root=f\"gs://{GOOGLE_CLOUD_GCS_BUCKET}/pipeline-root-rca\",\n",
    ")\n",
    "def telco_incident_prediction_pipeline():\n",
    "\n",
    "    load_events_task = load_data_from_bigquery_op(\n",
    "        project_id=GOOGLE_CLOUD_BIGQUERY_PROJECT,\n",
    "        query=f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_EVENTS}` TABLESAMPLE SYSTEM (10 PERCENT) \n",
    "        \"\"\",\n",
    "    )\n",
    "    load_incidents_task = load_data_from_bigquery_op(\n",
    "        project_id=GOOGLE_CLOUD_BIGQUERY_PROJECT,\n",
    "        query=f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{GOOGLE_CLOUD_BIGQUERY_PROJECT}.{GOOGLE_CLOUD_BIGQUERY_DATASET}.{BASE_TABLE_NAME_INCIDENTS}`\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    create_features_task = create_features_op(\n",
    "        input_data_path=load_events_task.outputs[\"output_data_path\"],\n",
    "        window_size=\"1h\",\n",
    "    )\n",
    "\n",
    "    join_features_task = join_features_op(\n",
    "        features_data_path=create_features_task.outputs[\"output_data_path\"],\n",
    "        incidents_data_path=load_incidents_task.outputs[\"output_data_path\"],\n",
    "    )\n",
    "\n",
    "    train_model_task = train_and_evaluate_model_op(\n",
    "        input_data_path=join_features_task.outputs[\"output_data_path\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telco Incident Prediction Pipeline\n",
    "\n",
    "This code defines a Kubeflow pipeline named \"telco-incident-prediction-pipeline\" for predicting incidents in a telecommunications network.\n",
    "\n",
    "### Pipeline Structure\n",
    "\n",
    "The pipeline consists of the following tasks:\n",
    "\n",
    "1. **`load_events_task`**\n",
    "\n",
    "   * Loads events data from a BigQuery table specified by `BASE_TABLE_NAME_EVENTS`. \n",
    "   * Uses a 10% systematic sample of the table to potentially speed up development and testing.\n",
    "\n",
    "2. **`load_incidents_task`**\n",
    "\n",
    "   * Loads incidents data from a BigQuery table specified by `BASE_TABLE_NAME_INCIDENTS`.\n",
    "   * Loads the entire table.\n",
    "\n",
    "3. **`create_features_task`**\n",
    "\n",
    "   * Takes the output of `load_events_task` (events data) as input.\n",
    "   * Creates features using a 1-hour window size.\n",
    "   * The specific feature engineering steps are defined within the `create_features_op` component (refer to previous explanations).\n",
    "\n",
    "4. **`join_features_task`**\n",
    "\n",
    "   * Takes the outputs of `create_features_task` (engineered features) and `load_incidents_task` (incidents data) as input.\n",
    "   * Joins the features with incident information to create a labeled dataset.\n",
    "   * The joining logic is defined within the `join_features_op` component.\n",
    "\n",
    "5. **`train_model_task`**\n",
    "\n",
    "   * Takes the output of `join_features_task` (labeled dataset) as input.\n",
    "   * Trains and evaluates a Random Forest Classifier model on the data.\n",
    "   * The model training and evaluation steps are defined within the `train_and_evaluate_model_op` component.\n",
    "\n",
    "### Pipeline Root\n",
    "\n",
    "* `pipeline_root`: Specifies the Google Cloud Storage (GCS) bucket where pipeline artifacts (metadata, intermediate outputs) will be stored.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "* **Data Loading:** The pipeline starts by loading events and incidents data from BigQuery.\n",
    "* **Feature Engineering:** Features are created from the events data using time-based aggregation and pairwise difference calculations.\n",
    "* **Data Labeling:** The engineered features are joined with incident information to create a labeled dataset for training a supervised model.\n",
    "* **Model Training and Evaluation:** A Random Forest Classifier is trained and evaluated on the labeled data.\n",
    "* **Vertex AI Pipelines:** This pipeline is designed to run on Vertex AI Pipelines, leveraging its capabilities for orchestration, scalability, and tracking of machine learning workflows.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Environment Variables:** The pipeline references environment variables (`GOOGLE_CLOUD_GCS_BUCKET`, `GOOGLE_CLOUD_BIGQUERY_PROJECT`, `GOOGLE_CLOUD_BIGQUERY_DATASET`, `BASE_TABLE_NAME_EVENTS`, `BASE_TABLE_NAME_INCIDENTS`) that need to be set appropriately before running the pipeline.\n",
    "* **Data Schema:** The pipeline assumes specific column names and data structures in the BigQuery tables and intermediate CSV files. Ensure that your data matches these expectations.\n",
    "* **Model Deployment:** This pipeline focuses on training and evaluating the model. Further steps would be needed to deploy the trained model for real-time incident prediction.\n",
    "\n",
    "Let me know if you have any specific questions or would like any part explained in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=telco_incident_prediction_pipeline,\n",
    "    package_path=\"telco_incident_prediction_pipeline.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this code submits the defined Kubeflow pipeline for execution on Vertex AI Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "DISPLAY_NAME = \"telco_incident_prediction_pipeline\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=f\"{DISPLAY_NAME}.yml\",\n",
    ")\n",
    "\n",
    "job.run(service_account=GOOGLE_CLOUD_SERVICE_ACCOUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
